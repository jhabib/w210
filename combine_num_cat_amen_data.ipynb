{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Numeric and Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files that we need to load\n",
    "\n",
    "- amenities_test_deps.csv\n",
    "- amenities_test_indeps.csv\n",
    "- amenities_train_deps.csv\n",
    "- amenities_train_indeps.csv\n",
    "\n",
    "- categorical_test_deps.csv\n",
    "- categorical_test_indeps.csv\n",
    "- categorical_train_deps.csv\n",
    "- categorical_train_indeps.csv\n",
    "\n",
    "- numeric_test_deps.csv\n",
    "- numeric_test_indeps.csv\n",
    "- numeric_train_deps.csv\n",
    "- numeric_train_indeps.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(filename, valtype):\n",
    "    df = pd.read_csv(filename, low_memory=False, dtype=valtype)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "def create_combined_df(input_dict):\n",
    "    fdf = pd.DataFrame()\n",
    "    cols = OrderedDict()\n",
    "    for k, v in input_dict.items():\n",
    "        df = read_df('./data/'+k, v)\n",
    "        colnames = [c for c in df.columns if c not in ['None', 'Unnamed: 0']]\n",
    "        cols[k] = colnames\n",
    "        fdf = pd.concat([fdf, df], axis=1)\n",
    "    \n",
    "    # fdf = fdf.DataFrame(fdf, columns=cols)\n",
    "    fdf = fdf.drop(['None', 'Unnamed: 0'], axis=1)\n",
    "    return fdf, cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127302, 195)\n",
      "(127302, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_scores_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28699</th>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_scores_rating\n",
       "28699                  95.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inpd = {\n",
    "    'numeric_train_deps.csv': float, \n",
    "    'categorical_train_deps.csv': int,\n",
    "    'amenities_train_deps.csv': int, \n",
    "}\n",
    "\n",
    "X_train, cols = create_combined_df(train_inpd)\n",
    "y_train = pd.read_csv('./data/numeric_train_indeps.csv', low_memory=False)\n",
    "y_train = y_train.drop(['Unnamed: 0'], axis=1).astype(float)\n",
    "y_train.sample()\n",
    "\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "X_train.sample()\n",
    "y_train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = cols['numeric_train_deps.csv']\n",
    "categorical_cols = cols['categorical_train_deps.csv']\n",
    "amen_cols = cols['amenities_train_deps.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_colnames = X_train.columns\n",
    "dependent_variable = y_train.columns[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62702, 195)\n",
      "(62702, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_scores_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46102</th>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_scores_rating\n",
       "46102                  80.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inpd = {\n",
    "    'numeric_test_deps.csv': float, \n",
    "    'categorical_test_deps.csv': int,\n",
    "    'amenities_test_deps.csv': int, \n",
    "}\n",
    "\n",
    "X_test, cols = create_combined_df(test_inpd)\n",
    "y_test = pd.read_csv('./data/numeric_test_indeps.csv', low_memory=False)\n",
    "y_test = y_test.drop(['Unnamed: 0'], axis=1).astype(float)\n",
    "y_test.sample()\n",
    "\n",
    "print X_test.shape\n",
    "print y_test.shape\n",
    "X_test.sample()\n",
    "y_test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_rand_idxs(start, end, size, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    return np.random.randint(start, end, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(X, y, Xt, yt, model):\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    if len(yt.shape) == 1:\n",
    "        yt = yt.reshape(-1, 1)\n",
    "    \n",
    "    res = {'train': {}, 'test': {}}\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    train_pred = model.predict(X)\n",
    "    res['train']['pred'] = train_pred\n",
    "    \n",
    "    test_pred = model.predict(Xt)\n",
    "    res['test']['pred'] = test_pred\n",
    "    \n",
    "    \n",
    "    for name, tup in zip(['train', 'test'], [(y, train_pred), (yt, test_pred)]):\n",
    "        act, prd = tup[0].ravel(), tup[1].ravel()\n",
    "        print 'Results for %s' % name\n",
    "        \n",
    "        mserr = mean_squared_error(act, prd)\n",
    "        res[name]['mse'] = mserr\n",
    "        print '%s Mean Squared Error: %.4f' % (name, mserr)\n",
    "        \n",
    "        rsq_score = r2_score(act, prd)\n",
    "        res[name]['r2'] = rsq_score\n",
    "        print '%s R-Squared: %.4f' % (name, rsq_score)\n",
    "        \n",
    "        plt.scatter(act, prd, alpha=0.5)\n",
    "        plt.title(name+': predictions vs. actual')\n",
    "        plt.show()\n",
    "        plt.scatter(act, act-prd, alpha=0.5)\n",
    "        plt.title(name+': residuals vs. actual')\n",
    "        plt.show()\n",
    "        plt.hist(act-prd, alpha=0.5)\n",
    "        plt.title(name+': residuals histogram')\n",
    "        plt.show()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "1. Remove 'bathrooms' >= 5\n",
    "2. Remove 'bedrooms' > 5\n",
    "3. Remove 'beds' >= 8\n",
    "4. Remove 'cleaning_fee' >= 400\n",
    "5. Remove 'guests_included' > 8\n",
    "6. Remove 'host_listings_count' > 100\n",
    "7. Remove 'host_acceptance_rate' == 0.\n",
    "8. Remove 'host_response_rate' == 0.\n",
    "9. Remove 'reviews_per_month' > 12.\n",
    "10. Drop 'host_has_profile_pic'\n",
    "11. Drop 'host_identity_verified'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing values for Train and Test sets\n",
    "We will use Median for imputing missing values because it is not as affected by outliers as the Mean.\n",
    "We will train the Imputer on the training data, and use this to fill the values for both Train and Test sets. \n",
    "We will not train a new Imputer on test data. This is important because we do not want to look at the test data when imputing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127302, 195)\n",
      "(62702, 195)\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values using median for both independent and dependent variables\n",
    "# We don't care for host_has_profile_pic, and host_identity_verified.\n",
    "# We dropped missing values from our dependent variable earlier, \n",
    "# so it won't be affected by imputation here.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# We need the \n",
    "train_colnames = X_train.columns\n",
    "\n",
    "# Train data first\n",
    "# axis 0 means impute along columns\n",
    "train_imp = Imputer(missing_values=np.nan, strategy='median', axis=0, copy=True)\n",
    "train_imp.fit(X_train)\n",
    "X_train_imp = train_imp.transform(X_train)\n",
    "\n",
    "\n",
    "# Then Test data\n",
    "test_colnames = X_test.columns\n",
    "X_test_imp = train_imp.transform(X_test)\n",
    "\n",
    "print X_train_imp.shape\n",
    "print X_test_imp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Outliers\n",
    "Drop them from Training data, as discussed in the Takeaways section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = list(train_colnames.values) + [dependent_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.DataFrame(X_train_imp, columns=train_colnames)\n",
    "Train = pd.concat([Train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = pd.DataFrame(X_test_imp, columns=test_colnames)\n",
    "Test = pd.concat([Test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62702, 196)\n"
     ]
    }
   ],
   "source": [
    "print Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119615, 196)\n",
      "(58911, 196)\n"
     ]
    }
   ],
   "source": [
    "def drop_outliers(t):\n",
    "    # Drop outliers from dependent variable\n",
    "    h = 0\n",
    "    if 'review_scores_value' == dependent_variable:\n",
    "        h = 3\n",
    "    elif 'review_scores_rating' == dependent_variable:\n",
    "        h = 80\n",
    "    t = t.loc[t[dependent_variable] >= h]\n",
    "    \n",
    "    # Remove reviews_per_month == 0\n",
    "    t = t.loc[t.reviews_per_month > 0].astype(float)\n",
    "\n",
    "    # 1. Remove 'bathrooms' >= 5, convert to int\n",
    "    t = t.loc[t.bathrooms < 5]\n",
    "    t.bathrooms = t.bathrooms.astype(int)\n",
    "\n",
    "    # 2. Remove 'bedrooms' > 5, convert to int\n",
    "    t = t.loc[t.bedrooms <= 5]\n",
    "    t.bedrooms = t.bedrooms.astype(int)\n",
    "\n",
    "    # 3. Remove 'beds' >= 8, convert to int\n",
    "    t = t.loc[t.beds < 8]\n",
    "    t.beds = t.beds.astype(int)\n",
    "\n",
    "    # 4. Remove 'cleaning_fee' > 400\n",
    "    t = t.loc[t.cleaning_fee <= 400]\n",
    "\n",
    "    # 5. Remove 'guests_included' > 8\n",
    "    t = t.loc[t.guests_included <= 8]\n",
    "    t = t.loc[t.guests_included > 0]\n",
    "    t.guests_included = t.guests_included.astype(int)\n",
    "\n",
    "    # 6. Remove 'host_listings_count' > 100\n",
    "    t = t.loc[t.host_listings_count <= 100]\n",
    "    # Remove'host_listings_count' <= 0\n",
    "    t = t.loc[t.host_listings_count > 0]\n",
    "    t.host_listings_count = t.host_listings_count.astype(int)\n",
    "    \n",
    "    \n",
    "\n",
    "    # 7. Remove 'host_acceptance_rate' == 0.\n",
    "    t = t.loc[t.host_acceptance_rate > 0.]\n",
    "\n",
    "    # 8. Remove 'host_response_rate' == 0.\n",
    "    t = t.loc[t.host_acceptance_rate > 0.]\n",
    "\n",
    "    # 9. Remove 'reviews_per_month' > 12.\n",
    "    t = t.loc[t.reviews_per_month <= 12]\n",
    "    \n",
    "    return t\n",
    "\n",
    "Train = drop_outliers(Train)\n",
    "# For Test, we will only drop the unneeded columns\n",
    "# Test = Test.drop(['host_has_profile_pic', 'host_identity_verified'], axis=1)\n",
    "\n",
    "Test = drop_outliers(Test)\n",
    "\n",
    "print Train.shape\n",
    "print Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,\n",
       "        91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Train.review_scores_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.,\n",
       "        91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Test.review_scores_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create feature interactions\n",
    "\n",
    "1. bathrooms x bedrooms\n",
    "2. bedrooms x beds\n",
    "3. beds x guests_included\n",
    "4. bathrooms x guests_included\n",
    "5. host_acceptance_rate x host_response_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction(f1, f2, intyp):\n",
    "    if 'D' == intype:\n",
    "        return f1 / f2\n",
    "    if 'A' == intype:\n",
    "        return f1 + f2\n",
    "    if 'S' == intype:\n",
    "        return f1 - f2\n",
    "    \n",
    "    # Mulitply the features by default\n",
    "    return f1 * f2\n",
    "\n",
    "def get_interacted_dataset(ds, interactions_dict, drop=False):\n",
    "    for k, v in interactions_dict.items():\n",
    "        ds[k+'_by_'+v[0]] = ds[k] * ds[v[0]]\n",
    "        \n",
    "    if drop:\n",
    "        for k in interactions_dict.keys():\n",
    "            ds = ds.drop(k, axis=1)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "inter_dct = {\n",
    "    'bathrooms': ('bedrooms', 'D'), \n",
    "    'beds': ('bedrooms', 'D'), \n",
    "    'beds': ('guests_included', 'M'), \n",
    "    'bathrooms': ('guests_included', 'D'), \n",
    "    'cleaning_fee': ('guests_included', 'D'), \n",
    "    'host_acceptance_rate': ('host_response_rate', 'D')\n",
    "}\n",
    "\n",
    "Train = get_interacted_dataset(Train, inter_dct, drop=False)\n",
    "Test = get_interacted_dataset(Test, inter_dct, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119615, 200)\n",
      "(58911, 200)\n"
     ]
    }
   ],
   "source": [
    "print Train.shape\n",
    "print Test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the Dependent and Independet Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119615,)\n",
      "(119615, 199)\n",
      "(58911,)\n",
      "(58911, 199)\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "y_train = Train[dependent_variable]\n",
    "print y_train.shape\n",
    "\n",
    "X_train = Train.drop([dependent_variable], axis=1)\n",
    "print X_train.shape\n",
    "\n",
    "# Test data\n",
    "y_test = Test[dependent_variable]\n",
    "print y_test.shape\n",
    "\n",
    "X_test = Test.drop([dependent_variable], axis=1)\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())\n",
    "\n",
    "X_train_num = normalize_df(X_train[numeric_cols])\n",
    "X_test_num = normalize_df(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119615, 12)\n",
      "(58911, 12)\n"
     ]
    }
   ],
   "source": [
    "print X_train_num.shape\n",
    "print X_test_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print type(X_train)\n",
    "print type(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the Numeric columns\n",
    "We don't want any one variable dominating others in the regression model, so let's start by scaling the dependent variables. As with Imputation, we will train the scaler only on the training data (to learn the mean and standard deviation), and then use it as-is on the Test data. We will assume that all dependent variables are interval."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scale the data\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "scaler = QuantileTransformer(output_distribution='normal') # output distribution: 'uniform', or 'normal'\n",
    "scaler.fit(X_train_num)\n",
    "X_train_num = scaler.transform(X_train_num)\n",
    "X_test_num = scaler.transform(X_test_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = pd.DataFrame(X_train_num, columns=numeric_cols)\n",
    "X_test_num = pd.DataFrame(X_test_num, columns=numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write back all data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train, columns=[dependent_variable])\n",
    "y_test = pd.DataFrame(y_test, columns=[dependent_variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.to_csv('./data/fin_num_train_deps.csv', encoding='utf8')\n",
    "y_train.to_csv('./data/fin_train_indeps.csv', encoding='utf8')\n",
    "X_test_num.to_csv('./data/fin_num_test_deps.csv', encoding='utf8')\n",
    "y_test.to_csv('./data/fin_test_indeps.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = X_train[categorical_cols].astype(int)\n",
    "X_test_cat = X_test[categorical_cols].astype(int)\n",
    "\n",
    "X_train_cat.to_csv('./data/fin_cat_train_deps.csv', encoding='utf8')\n",
    "X_test_cat.to_csv('./data/fin_cat_test_deps.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write amenities columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_amen = X_train[amen_cols].astype(int)\n",
    "X_test_amen = X_test[amen_cols].astype(int)\n",
    "\n",
    "X_train_amen.to_csv('./data/fin_amen_train_deps.csv', encoding='utf8')\n",
    "X_test_amen.to_csv('./data/fin_amen_test_deps.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((119615, 12), (119615, 124), (119615, 59))\n",
      "((58911, 12), (58911, 124), (58911, 59))\n"
     ]
    }
   ],
   "source": [
    "print (X_train_num.shape, X_train_cat.shape, X_train_amen.shape)\n",
    "print (X_test_num.shape, X_test_cat.shape, X_test_amen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined Train and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = pd.read_csv('./data/fin_num_train_deps.csv')\n",
    "num_train = num_train.drop(['Unnamed: 0'], axis=1)\n",
    "num_test = pd.read_csv('./data/fin_num_test_deps.csv')\n",
    "num_test = num_test.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "cat_train = pd.read_csv('./data/fin_cat_train_deps.csv')\n",
    "cat_train = cat_train.drop(['Unnamed: 0'], axis=1)\n",
    "cat_test = pd.read_csv('./data/fin_cat_test_deps.csv')\n",
    "cat_test = cat_test.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "amen_train = pd.read_csv('./data/fin_amen_train_deps.csv')\n",
    "amen_train = amen_train.drop(['Unnamed: 0'], axis=1)\n",
    "amen_test = pd.read_csv('./data/fin_amen_test_deps.csv')\n",
    "amen_test = amen_test.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((119615, 12), (119615, 124), (119615, 59))\n"
     ]
    }
   ],
   "source": [
    "print (num_train.shape, cat_train.shape, amen_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comb = pd.concat([num_train, cat_train, amen_train], \n",
    "                         axis=1)\n",
    "X_test_comb = pd.concat([num_test, cat_test, amen_test], \n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119615, 195) (119615,)\n",
      "(58911, 195) (58911,)\n"
     ]
    }
   ],
   "source": [
    "print X_train_comb.shape, y_train.shape\n",
    "print X_test_comb.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comb.to_csv('./data/fin_comb_train_deps.csv', encoding='utf8')\n",
    "X_test_comb.to_csv('./data/fin_comb_test_deps.csv', encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
